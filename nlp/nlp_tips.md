```python

```

```python

```

```python

```

```python

```

## 数据相关

- 巧妇难为无米之炊嘛，所以，当数据量不够的时候，数据增强也是很有必要的


### 数据增强
- 一般包括，词汇替换、回译

| 发方法名称 | 方法定义 | 相关资源
| :- |:- |:- 
| 词汇替换 | 同义词（查词典 or 向量表征相近）的词汇替换；<br>tf-idf将得分较低的词替换掉 | Python的Gensim
| 回译 | 将文本翻译成其他1-2种语言，再翻译回原语言 | 百度/google翻译API
| 文本表面转换 | 在英语中，将动词形式由简写转化为完整形式或者反过来的 | 
| 引入噪声数据 | 引入随机噪声、或者常见拼写错误、或者将句子顺序打乱
| 迁移学习 | 借用一个在大规模数据集上预训练好的通用模型（bert），并在针对目标任务的小数据集上进行微调（fine-tune）|

<!-- #region -->
#### 词汇替换
- （1）基于词典的替换
    - 种技术中，我们从句子中随机取出一个单词，并使用同义词词典将其替换为同义词。
    - 可以使用WordNet的英语词汇数据库来查找同义词，然后执行替换。


- （2）基于词向量的替换
    - 我们采用预先训练好的单词嵌入，如Word2Vec、GloVe、FastText、Sent2Vec，并**使用嵌入空间中最近的相邻单词替换句子中的某些单词。**
    - 例如，你可以用三个最相似的单词来替换句子中的单词，并得到文本的三个变体。
    
    
- （3）预训练模型
    - 像BERT、ROBERTA和ALBERT这样的Transformer模型已经接受了大量的文本训练，使用一种称为“Masked LanguageModeling”的预训练，即模型必须根据上下文来预测遮盖的词汇。
    - 这可以用来扩充一些文本。
    
    
- （4）基于TF-IDF的词替换
    - 其基本思想是，TF-IDF分数较低的单词不能提供信息，因此可以在不影响句子的ground-truth的情况下替换它们。
<!-- #endregion -->

<!-- #region -->
#### 回译
- 反向翻译
    - （1）把一些句子(如英语)翻译成另一种语言，如法语
    - （2）将法语句子翻译回英语句子。
    - （3）检查新句子是否与原来的句子不同。如果是，那么我们使用这个新句子作为原始文本的数据增强。
    

- 翻译平台
    - 谷歌翻译(google)，语言支持最多
    - 百度翻译(baidu)，国内支持翻译语言最多的了(28种互译)，注册账户后每月有200万字符的流量，大约2M，超出则49元人民币/百万字符
    - 有道翻译(youdao)，有100元体验金，支持11种语言，48元/百万字符
    - 搜狗翻译(sougou)，78种语言，200元体验金，常见语言40元/百万字符,非常见语言60元/百万字符
    - 腾讯翻译(tencent)，接口变动多
    - 必应翻译(bing)，微软开发的
<!-- #endregion -->

#### 文本表面转换
- 对于英语而言
    - 使用正则表达式的简单的模式匹配的转换，比如，**将动词形式由简写转化为完整形式或者反过来的**。
    - 我们可以通过这个来生成增强型文本。

<!-- #region -->
#### 引入噪声数据
- 随机噪声注入
    - 这些方法的思想是在文本中加入噪声，使所训练的模型对扰动具有鲁棒性
    
    
- 拼写错误注入
    - 在这种方法中，我们在句子中的一些随机单词上添加拼写错误。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
    
    
- 句子打乱
    - 这是一种朴素的技术，我们将训练文本中的句子打乱，以创建一个增强版本
<!-- #endregion -->

## 训练技巧

<!-- #region -->
### 过拟合问题

    
    
现象 | 情况说明 | 如何判断 | 带来的影响 | 哪些模型会出现 | 如何解决 |
:- |:- |:- |:- | -------------- | :-
欠拟合 | 模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律 |训练集和验证集的准确率都很低，那么说明模型欠拟合（高方差，高偏差）  | | | 可以通过增加网络复杂度或者在模型中增加特征、boosting方法
[过拟合](#tag71) | 模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差 | 在训练集上表现好，但在测试集和验证集上表现很差（高方差、低偏差）| |样本不足、数据太脏、模型太复杂 | 解决过拟合的根本性方法(数据增强或者说获取更多的数据集)、选择合适的模型降低模型复杂度、减少特征、[DROPOUT](#tag72)等bagging方法、[early stopping](#tag73)
<!-- #endregion -->

#### 解决方法总结
- 一、神经网络，解决过拟合问题，一般是以以下几种方式进行的，
    - 1.正则化（参数范数惩罚L1,L2）
    - 2.数据增强（通过对样本进行各种操作：图片的形变、位移等，音频的声调、音量等、文本的反序）
    - 3.提前终止（early stopping）
    - 4.dropout（在深度学习中最为常用的手段，其实就是随机将一部分神经元失活）
    - 5.batch normalization（深度学习中对每一批的输入数据进行标准归一化操作）
    - 6.bagging等模型集成的方法（通过组合多个模型减少泛化误差）

<!-- #region -->
- 二、为什么会出现过拟合现象<a id="tag71"></a>：
    - 1、训练数据集样本单一，样本不足。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。
    - 2、训练数据中噪声干扰过大。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。
    - 3、模型过于复杂。模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。


- 三、如何防止过拟合？
    - 常用的正则化方法根据具体的使用策略不同可分为
    - 1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；
    - 2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early stopping)和Dropout；
    - 3）不直接提供约束的隐式正则化方法，如数据增强等。
    - 4) 采用合适的模型
    
    
- 四、正则化
    - L1正则化：全部权重$w$的绝对值的和，再乘以$\frac{\lambda}{n}$，此时损失函数变成了：
    $$C = C_0 + \frac{\lambda}{n}\sum_i|W_i|$$
    **L1正则化使得权重$W$往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。**
    这也就是L1正则化会产生更稀疏（sparse）的解的原因。此处稀疏性指的是最优值中的一些参数为0。L1正则化的稀疏性质已经被广泛地应用于特征选择机制，从可用的特征子集中选择出有意义的特征。
    - L2 正则化：全部权重$w$的平方和，再乘以$\frac{\lambda}{2n}$：
    $$C = C_0 + \frac{\lambda}{2n}\sum_i|W_i^2|$$
    L2正则化起到使得权重参数$W$变小的效果，为什么能防止过拟合呢？**因为更小的权重参数$W$意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。**


- 五、Dropout<a id="tag72"></a>：
    - Dropout是在训练网络时用的一种技巧（trike），相当于在隐藏单元增加了噪声。**Dropout 指的是在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）**。所谓的“删除”不是真正意义上的删除，其实就是将该部分神经元的激活函数设为0（激活函数的输出为0），让这些神经元不计算而已。
    - 为什么有效：Dropout为什么有助于防止过拟合呢？
    - （a）在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果。随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。
    - （b）它消除或者减弱了神经元节点间的联合，降低了网络对单个神经元的依赖，从而增强了泛化能力。


-  六、Early stopping<a id="tag73"></a>：
    - 对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）。Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。
    - Early stopping旨在解决epoch数量需要手动设置的问题。具体做法：**每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数。**
    - 为什么能防止过拟合？
        - 当还未在神经网络运行太多迭代过程的时候，w参数接近于0，因为随机初始化w值的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。
    - Early Stopping缺点：
        - 没有采取不同的方式来解决优化损失函数和过拟合这两个问题，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合。
<!-- #endregion -->

### 梯度消失/爆炸处理问题处理
- 说明
    - 梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。

<!-- #region -->
- 概览
    
现象 | 情况说明 | 如何判断 | 带来的影响 | 哪些模型会出现 | 如何解决 |
:- |:- |:- |:- | -------------- | :-
[梯度消失](#tag1) | 反向传播过程中，越靠前的层，梯度越来越小 |  | 结果不准确、学习时间长 | 应用梯度的方法训练的神经网络中：一是在深层网络中，二是采用了不合适的损失函数 | 激活函数、LSTM，ResNet
[梯度爆炸](#tag2) | 反向传播过程中，越靠前的层，梯度越来越大 | ①模型不稳定，导致更新过程中的损失出现显著变化；<br>②训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等 | 梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定 | 一般出现在深层网络和权值初始化值太大的情况下 | 梯度剪切、权值正则化、激活函数
<a id="tag9"></a>神经元坏死现象 | 这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新| | | 参数初始化问题；learning rate太高导致在训练过程中参数更新太大。| 采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法
[激活函数](#tag8) | 将非线性特性引入到我们的网络中,非线性的 | | 使得神经网络可以任意逼近任何非线性函数 | 输出层可能会使用线性激活函数，但在**隐含层都使用非线性激活函数。**|

#### 梯度消失    
- 梯度消失<a id="tag1"></a>[1^]
    - 常常发生在用**基于梯度的方法训练神经网络**的过程中。
    - 当我们在做反向传播，计算损失函数对权重的梯度时，随着越向后传播，梯度变得越来越小，这就意味着在网络的前面一些层的神经元，会比后面的训练的要慢很多，甚至不会变化。
    - 【梯度消失】经常出现，产生的原因有：一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。


- 梯度消失的影响
    - 网络的前面的一些层是很重要的，它们负责学习和识别简单的模式，也是整个网络的基础，如果他们的结果不准确的话，那么后面层**结果也会不准确**。
    - 而且用基于梯度的方法训练出参数，主要是通过学习参数的很小的变化对网络的输出值的影响有多大。如果参数的改变，网络的输出值贡献很小，那么就会**很难学习参数**，花费时间会非常长。


- 为什么会出现
    - 在训练神经网络时，为了让损失函数越来越小，其中一种优化的方法是梯度下降。
    - 梯度下降法简单的来说就是在**权重的负梯度方向更新权重**，如下面这个公式(1)所示，一直到梯度收敛为零。（当然在实际过程中，会通过设定一个超参数叫做最大跌代数来控制，如果迭代次数太小，结果就会不准确，如果迭代次数太大，那么训练过程会非常长。）
    - 这里就需要计算参数的梯度，方法是用反向传播：
    $$repeat \space until \frac{\partial{J}}{\partial{W^{layer}_{ij}}} \rightarrow 0 \\ W^{layer}_{ij} := W^{layer}_{ij} - \alpha\frac{\partial{J}}{\partial{W^{layer}_{ij}}} \tag{1}$$
    - 越靠前的层数，由于离损失越远，梯度计算式中包含的激活函数的导数就越多，那么训练也就越慢。


- **解决方法**
    - 逐层“预训练”（pre-training）＋对整个网络进行“[微调](#tag3)”（fine-tunning）
    - 选择合适的激活函数
    - batch normalization 批规范化：通过对每一层的输出规范为均值和方差一致的方法，消除了 w 带来的放大缩小的影响
    - [残差结构](#tag6)
    - [LSTM](#tag7)
        - 因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。
<!-- #endregion -->

<!-- #region -->
#### 梯度爆炸
- 梯度爆炸<a id="tag2"></a>[2^]
    - 【梯度爆炸】一般出现在深层网络和权值初始化值太大的情况下。在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。


- 解决方法
    - [梯度剪切](#tag4)（ Gradient Clipping）
    - [权重正则化](#tag5)
    - 选择合适的激活函数
    - batch normalization 批规范化，
    - RNN 的 truncated Backpropagation through time ，LSTM

#### 解决方法明细
-  1)pre-training+fine-tunning<a id="tag3"></a>
    - 采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；
    - 在预训练完成后，再对整个网络进行“微调”（fine-tunning）。
    - 此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

- 2） 梯度剪切：对梯度设定阈值<a id="tag4"></a>
    - 梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

- 3） 权重正则化<a id="tag5"></a>
    - 另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization），正则化主要是通过对网络权重做正则来限制过拟合。
    - 如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。比较常见的
    是 L1 正则和 L2 正则，在各个深度框架中都有相应的API可以使用正则化。
- 4） 选择relu等梯度大部分落在常数上的激活函数
    - relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。<a id="tag8"></a>
- 5） batch normalization
    - BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。
- 6） 残差网络的捷径<a id="tag6"></a>（shortcut）
    - 相比较于以前直来直去的网络结构，残差中有很多这样（如上图所示）的跨层连接结构，这样的结构在反向传播中具有很大的好处，可以避免梯度消失。
- 7） LSTM的<a id="tag7"></a>"门（gate）"结构
    - LSTM全称是长短期记忆网络（long-short term memory networks），LSTM的结构设计可以改善RNN中的梯度消失的问题。
    - **主要原因在于LSTM内部复杂的“门”(gates)。**
- 8）MAXOUT <a id="tag10"></a>
    - Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层
    - 我们假设网络某一层的输入特征向量为： $X=(x_1,x_2,……x_d)$，也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下：
    $$h_i(x) = \max_{j\in [1,k]}Z_{ij},其中，\color{blue}{k就是maxout层所需要的参数了，由我们人为设定大小。}$$
    - 公式中Z的计算公式为：
    $$Z_{ij} = x^{T}W_{\dots ij} + b_{ij}$$
    权重w是一个大小为(d,m,k)三维矩阵，b是一个大小为(m,k)的二维矩阵，这两个就是我们需要学习的参数。如果我们设定参数k=1，那么这个时候，网络就类似于以前我们所学普通的MLP网络。
    - 我们可以这么理解，本来传统的MLP算法在第i层到第i+1层，参数只有一组，然而现在我们不怎么干了，**我们在这一层同时训练n组参数，然后选择激活值最大的作为下一层神经元的激活值。**
    - Maxout是通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的边界，然后每一段取最大。
<!-- #endregion -->

<!-- #region -->
### OOV问题

- 1. Mixed Word/Character Model
    - 即把所有的OOV词，拆成字符。比如Jessica，变成J，e，s，s，i，c，a。其中是Begin，Middle，End的标记。
    - 这样处理的好处就是消灭了全部的OOV。坏处就是文本序列变得非常长，对于性能敏感的系统，这是难以接受的维度增长。


- 2. Wordpiece Model（WPM）
    - 和上面一样，同样要进行拆词。不同的是，非OOV的词也要拆，并且非字符粒度，而是sub-word。


- 3. UNK处理
    - 在训练数据充足的情况下，RNN模型可以轻松支持30k-80k的词表。**在大多数情况下，扩大词表都是首选的方案**。
    - 经过WPM处理后，配合词表加大，剩下的OOV都是冷门的长尾词。
        - 如果你不关注这部分性能，可以直接扔掉OOV词，删掉包含OOV的数据。
        - 对于分类型任务，就全部替换成标签。
        - 对于生成型任务，有不同的细节处理方案，可以看下经典的《Addressing the Rare Word Problem in Neural Machine Translation》，里面介绍了Copyable、PosALL和PosUNK三种替换策略。这类策略对于实体类NER的词，有比较好的效果。


- 4. 扩大词表
    - 终极解决办法。通常情况不使用大词表，一方面是因为训练数据的多样性有限，另一方面是softmax的计算速度受限。
    - 对于第一种情况，扩大语料范围。
    - 对于第二种情况，相关的加速策略可以将词表扩大10倍而GPU上的预测速度只降低一半（从5W词到50W词）
<!-- #endregion -->

<!-- #region -->
### 训练加速方式

- （1）内部方法
    - 网络结构的选择：比如CNN和RNN，CNN更适合并行结构；
    - 优化算法的改进：动量，自适应学习率；
    - 减少参数规模：比如使用GRU代替LSTM
    - 参数初始化：Batch Normalization
    - Mini-batch的调整，调大一些


- （2）外部方法
    - GPU加速训练
    - 数据并行：数据并行是指对数据数据做切分，同时采用多个模型实例，对多个分片的数据进行并行训练。
    - 模型并行：是指将模型拆分成几个分片，由几个训练单元分别持有，共同协助训练。
    - 同步模式：所有训练程序同一个批次的训练数据，完成后经过同步，再同时交换参数。参数交换完后所有的训练程序具有新的模型作为起点，再训练下一批次。
        - 需要用一个参数服务器加速训练。
    - 混合数据并行与模型并行
    - 结合GPU计算和集群计算技术，构建GPU集群正在成为加速大规模深度神经网络训练的有效解决方案。
<!-- #endregion -->

#### 调参技巧

1. 神经网络有什么调参技巧？
    - 哪些参数可以调？
        - 网络设计相关参数：网络层数、不同层的搭建顺序、隐藏层神经元的参数设置、loss的选择、正则化参数
        - 训练过程相关参数：**网络权重初始化方法、学习率、迭代次数、batch_size。**
    - 什么时候需要调参？
        - 欠拟合：优化数据集(数据清洗)、增加训练迭代次数、添加更多的层，增大神经元参数等
        - 过拟合：增加样本数量(数据增强)，加正则化参数(dropout)，使用早停机制等


#### 正则化

| 分类 | 定义 | 使用场景 | 所属分布
|:- |:- |:- |:-
| L1范数 | 为x向量各个元素绝对值之和。| L1范数可以使权值稀疏，方便特征提取 | L1是拉普拉斯分布
| L2范数 | 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数 | L2范数可以防止过拟合，提升模型的泛化能力 | L2是高斯分布
| Lp范数 | 为x向量各个元素绝对值p次方和的1/p次方.


### 训练技巧

1. 要做梯度归一化,即算出来的梯度除以minibatch size
2. clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15
    - 训 RNN 不加 gradient clipping，导致训练一段时间以后 loss 突然变成 Nan。
3. dropout对小数据防止过拟合有很好的效果,值一般设为0.5,**小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显**.因此可能的话，建议一定要尝试一下。 
    - dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.
    - 关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329
4. 优化器优先用adam，学习率设1e-3或1e-4，再试Radam（LiyuanLucasLiu/RAdam）。不推荐sgdm，因为很慢。
    - **adam收敛虽快但是得到的解往往没有sgd+momentum得到的解更好，如果不考虑时间成本的话还是用sgd吧**
    - adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。
    - 如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 
    - 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.
    - **据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。**
5. 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.
    - 1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。
    - 2. 输入0均值，sigmoid函数的输出不是0均值的。
6. rnn的dim和embdding size,
    - 一般从128上下开始调整. batch size,一般从128左右开始调整.
    - batch size合适最重要,并不是越大越好.
7. word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.尽量对数据做shuffle
8. LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 
    - 我这里实验设成1.0,可以提高收敛速度.
    - 实际使用中,不同的任务,可能需要尝试不同的值.
9. Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift
10. 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，
    - 原理很简单，就是给输出加了一个gate来控制信息的流动，
    - 详细介绍请参考论文: http://arxiv.org/abs/1505.00387来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。
11. 不管什么模型，先在一个较小的训练集上train和test，看看它能不能过拟合
    - 如果不能过拟合，可能是学习率太大，或者代码写错了。先调小学习率试一下，如果还不行就去检查代码
    - 先看dataloader输出的数据对不对，再看模型每一步的size是否符合自己期待。
    - 在数据集很大的情况下，别一上来就跑全量数据。建议先用 1/100、1/10 的数据跑一跑，对模型性能和训练时间有个底，外推一下全量数据到底需要跑多久。在没有足够的信心前不做大规模实验。
12. 看train/eval的loss曲线，
    - 正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。
    - 如果loss曲线非常不正常，很有可能是数据处理出了问题，比如label对应错了，回去检查代码。
13. 激活函数用relu一般就够了，也可以试试leaky relu
14. embedding层的embedsize可以小一些（64 or 128），之后LSTM或CNN的hiddensize要稍微大一些（256 or 512）
15. GRU和LSTM在大部分任务上效果差不多
16. 对于大多数任务，数据比模型重要。
    - 面对新任务时先分析数据，再根据数据设计模型，并决定各个参数。
    - **例如nlp有些任务中的padding长度，通常需要达到数据集的90%以上，可用pandas的describe函数进行分析。**

<!-- #region -->
## debug方法说明

- 可参考的步骤
    - 检查数据（输入的输入、label、padding是否OK）
    - 判断模型（eg，直接debug或者更换简单的模型）
    - 减少数据量训练（eg，让模型过拟合学习）然后来预测结果
    - 考虑正则化、数据增强等


- 现象：梯度在变化，损失也在下降，看似一切正常。
- 但是预测结果出来了：全部都是零值，或者预测全是正样本，生成全是：啊啊啊啊啊
    - 检查流程：
        - 1. 把复杂的模型首先替换成为简单的模型, 确认是否是模型的问题（eg，transformer --> lstm) 
        - 2. 去掉暂时不必要的数据预处理环节。 （eg，正则化，数据增强模块）
        - 3. 验证输入数据的正确性（eg，数据大小，padding, index)
        - 4. 从较小的数据集开始，等数据过拟合之后，利用训练样本进行测试（eg，可以先拿小数据集来debug）
        - 5. 加入之前忽略的项，（eg，正则，自定义损失等）


- 数据集的问题
    - 再次检查输入数据。 例如混淆了batch , length 的维度，打印或显示一批，确保正确。一般是$[batch, length, hidden_{dim}]$
    - 随机输入维度相同的数据，看是否产生错误的方式一样，如果是，说明在某些时候你的网络把数据转化为了垃圾。试着逐层调试，并查看出错的地方。
    - 确保输入样本与输出的匹配性（eg，给模型的标签是不是同你希望模型输出的一致）
    - 确认是否太多噪音 ，噪音太多很难用dl学习（eg，标签的错误）
    - 打乱数据集（eg，shuffle）
    - 是否类别失衡（eg，需要做解决类别失衡的问题）
    - 确保数据集不是单一的标签（eg，标签全为1啦）
    - 减少训练的batch size 到合适的大小（eg，8/16/32/64）


- 数据归一化
    - 归一化特征（**大部分的神经网络，都假设输入和输出数据都以一个约是1的标准差和约是0的均值分布**，涉及权重初始化、激活函数到训练网络的优化算法。）
    - 模型数据是否足够（是否一直欠拟合）


- 实现的问题
    - 试着解决问题的更简单的版本（eg，问题的可解决性，以及后期的可优化性）
    - 检查损失函数 （eg，自己实现的损失函数，增加测试单元验证其正确性）
    - 核实损失函数输入要求（eg，with softmax or without softmax)
    - 利用其他指标来监控模型（eg，精度等）
    - 检查是否在一些层无意中阻止了梯度的更新
    - 扩大网络规模
    - 探索维度误差（eg，维度$hidden_{dim}$给的太小，不足以保存信息）


- 训练问题
    - 使用一个真正小的数据集，确保能正常工作
    - 检查权重的初始化，使用Xavier或者其他初始化
    - 改变超参
    - 减小正则化
    - 尝试使用不同的优化器 ，例如从Adam换到SGD
    - 梯度爆炸或者消失（监控梯度，同时进行梯度截断）
    - 增加减小学习率
    - 神经网络梯度太深
<!-- #endregion -->
