## 数据相关

- 巧妇难为无米之炊嘛，所以，当数据量不够的时候，数据增强也是很有必要的


### 数据增强
- 一般包括，词汇替换、回译

| 发方法名称 | 方法定义 | 相关资源
| :- |:- |:- 
| 词汇替换 | 同义词（查词典 or 向量表征相近）的词汇替换；<br>tf-idf将得分较低的词替换掉 | Python的Gensim
| 回译 | 将文本翻译成其他1-2种语言，再翻译回原语言 | 百度/google翻译API
| 文本表面转换 | 在英语中，将动词形式由简写转化为完整形式或者反过来的 | 
| 引入噪声数据 | 引入随机噪声、或者常见拼写错误、或者将句子顺序打乱
| 迁移学习 | 借用一个在大规模数据集上预训练好的通用模型（bert），并在针对目标任务的小数据集上进行微调（fine-tune）|

<!-- #region -->
#### 词汇替换
- （1）基于词典的替换
    - 种技术中，我们从句子中随机取出一个单词，并使用同义词词典将其替换为同义词。
    - 可以使用WordNet的英语词汇数据库来查找同义词，然后执行替换。


- （2）基于词向量的替换
    - 我们采用预先训练好的单词嵌入，如Word2Vec、GloVe、FastText、Sent2Vec，并**使用嵌入空间中最近的相邻单词替换句子中的某些单词。**
    - 例如，你可以用三个最相似的单词来替换句子中的单词，并得到文本的三个变体。
    
    
- （3）预训练模型
    - 像BERT、ROBERTA和ALBERT这样的Transformer模型已经接受了大量的文本训练，使用一种称为“Masked LanguageModeling”的预训练，即模型必须根据上下文来预测遮盖的词汇。
    - 这可以用来扩充一些文本。
    
    
- （4）基于TF-IDF的词替换
    - 其基本思想是，TF-IDF分数较低的单词不能提供信息，因此可以在不影响句子的ground-truth的情况下替换它们。
<!-- #endregion -->

#### 回译
- 反向翻译
    - （1）把一些句子(如英语)翻译成另一种语言，如法语
    - （2）将法语句子翻译回英语句子。
    - （3）检查新句子是否与原来的句子不同。如果是，那么我们使用这个新句子作为原始文本的数据增强。


#### 文本表面转换
- 对于英语而言
    - 使用正则表达式的简单的模式匹配的转换，比如，**将动词形式由简写转化为完整形式或者反过来的**。
    - 我们可以通过这个来生成增强型文本。

<!-- #region -->
#### 引入噪声数据
- 随机噪声注入
    - 这些方法的思想是在文本中加入噪声，使所训练的模型对扰动具有鲁棒性
    
    
- 拼写错误注入
    - 在这种方法中，我们在句子中的一些随机单词上添加拼写错误。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
    
    
- 句子打乱
    - 这是一种朴素的技术，我们将训练文本中的句子打乱，以创建一个增强版本
<!-- #endregion -->
