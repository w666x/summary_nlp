## 数据相关

- 巧妇难为无米之炊嘛，所以，当数据量不够的时候，数据增强也是很有必要的


### 数据增强
- 一般包括，词汇替换、回译

| 发方法名称 | 方法定义 | 相关资源
| :- |:- |:- 
| 词汇替换 | 同义词（查词典 or 向量表征相近）的词汇替换；<br>tf-idf将得分较低的词替换掉 | Python的Gensim
| 回译 | 将文本翻译成其他1-2种语言，再翻译回原语言 | 百度/google翻译API
| 文本表面转换 | 在英语中，将动词形式由简写转化为完整形式或者反过来的 | 
| 引入噪声数据 | 引入随机噪声、或者常见拼写错误、或者将句子顺序打乱
| 迁移学习 | 借用一个在大规模数据集上预训练好的通用模型（bert），并在针对目标任务的小数据集上进行微调（fine-tune）|

<!-- #region -->
#### 词汇替换
- （1）基于词典的替换
    - 种技术中，我们从句子中随机取出一个单词，并使用同义词词典将其替换为同义词。
    - 可以使用WordNet的英语词汇数据库来查找同义词，然后执行替换。


- （2）基于词向量的替换
    - 我们采用预先训练好的单词嵌入，如Word2Vec、GloVe、FastText、Sent2Vec，并**使用嵌入空间中最近的相邻单词替换句子中的某些单词。**
    - 例如，你可以用三个最相似的单词来替换句子中的单词，并得到文本的三个变体。
    
    
- （3）预训练模型
    - 像BERT、ROBERTA和ALBERT这样的Transformer模型已经接受了大量的文本训练，使用一种称为“Masked LanguageModeling”的预训练，即模型必须根据上下文来预测遮盖的词汇。
    - 这可以用来扩充一些文本。
    
    
- （4）基于TF-IDF的词替换
    - 其基本思想是，TF-IDF分数较低的单词不能提供信息，因此可以在不影响句子的ground-truth的情况下替换它们。
<!-- #endregion -->

#### 回译
- 反向翻译
    - （1）把一些句子(如英语)翻译成另一种语言，如法语
    - （2）将法语句子翻译回英语句子。
    - （3）检查新句子是否与原来的句子不同。如果是，那么我们使用这个新句子作为原始文本的数据增强。


#### 文本表面转换
- 对于英语而言
    - 使用正则表达式的简单的模式匹配的转换，比如，**将动词形式由简写转化为完整形式或者反过来的**。
    - 我们可以通过这个来生成增强型文本。

<!-- #region -->
#### 引入噪声数据
- 随机噪声注入
    - 这些方法的思想是在文本中加入噪声，使所训练的模型对扰动具有鲁棒性
    
    
- 拼写错误注入
    - 在这种方法中，我们在句子中的一些随机单词上添加拼写错误。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
    
    
- 句子打乱
    - 这是一种朴素的技术，我们将训练文本中的句子打乱，以创建一个增强版本
<!-- #endregion -->

## 训练技巧


### 过拟合问题


#### 解决方法总结
- 神经网络，解决过拟合问题，一般是以以下几种方式进行的，
    - 1.正则化（参数范数惩罚L1,L2）
    - 2.数据增强（通过对样本进行各种操作：图片的形变、位移等，音频的声调、音量等、文本的反序）
    - 3.提前终止（early stopping）
    - 4.dropout（在深度学习中最为常用的手段，其实就是随机将一部分神经元失活）
    - 5.batch normalization（深度学习中对每一批的输入数据进行标准归一化操作）
    - 6.bagging等模型集成的方法（通过组合多个模型减少泛化误差）


### 梯度消失/爆炸处理问题处理

<!-- #region -->
## debug方法说明

- 可参考的步骤
    - 检查数据（输入的输入、label、padding是否OK）
    - 判断模型（eg，直接debug或者更换简单的模型）
    - 减少数据量训练（eg，让模型过拟合学习）然后来预测结果
    - 考虑正则化、数据增强等


- 深度学习常见潜在的
- 如何去排查可能存在的bug
- 如何利用ide( pycharm ) debug
- 帮助模型设置参数的,训练的工具


- 现象：梯度在变化，损失也在下降，看似一切正常。
- 但是预测结果出来了：全部都是零值，或者预测全是正样本，生成全是：啊啊啊啊啊
    - 检查流程：
        - 1. 把复杂的模型首先替换成为简单的模型, 确认是否是模型的问题（eg，transformer --> lstm) 
        - 2. 去掉暂时不必要的数据预处理环节。 （eg，正则化，数据增强模块）
        - 3. 验证输入数据的正确性（eg，数据大小，padding, index)
        - 4. 从较小的数据集开始，等数据过拟合之后，利用训练样本进行测试（eg，可以先拿小数据集来debug）
        - 5. 加入之前忽略的项，（eg，正则，自定义损失等）


- 数据集的问题
    - 再次检查输入数据。 例如混淆了batch , length 的维度，打印或显示一批，确保正确。一般是$[batch, length, hidden_{dim}]$
    - 随机输入维度相同的数据，看是否产生错误的方式一样，如果是，说明在某些时候你的网络把数据转化为了垃圾。试着逐层调试，并查看出错的地方。
    - 确保输入样本与输出的匹配性（eg，给模型的标签是不是同你希望模型输出的一致）
    - 确认是否太多噪音 ，噪音太多很难用dl学习（eg，标签的错误）
    - 打乱数据集（eg，shuffle）
    - 是否类别失衡（eg，需要做解决类别失衡的问题）
    - 确保数据集不是单一的标签（eg，标签全为1啦）
    - 减少训练的batch size 到合适的大小（eg，8/16/32/64）


- 数据归一化
    - 归一化特征（**大部分的神经网络，都假设输入和输出数据都以一个约是1的标准差和约是0的均值分布**，涉及权重初始化、激活函数到训练网络的优化算法。）
    - 模型数据是否足够（是否一直欠拟合）


- 实现的问题
    - 试着解决问题的更简单的版本（eg，问题的可解决性，以及后期的可优化性）
    - 检查损失函数 （eg，自己实现的损失函数，增加测试单元验证其正确性）
    - 核实损失函数输入要求（eg，with softmax or without softmax)
    - 利用其他指标来监控模型（eg，精度等）
    - 检查是否在一些层无意中阻止了梯度的更新
    - 扩大网络规模
    - 探索维度误差（eg，维度$hidden_{dim}$给的太小，不足以保存信息）


- 训练问题
    - 使用一个真正小的数据集，确保能正常工作
    - 检查权重的初始化，使用Xavier或者其他初始化
    - 改变超参
    - 减小正则化
    - 尝试使用不同的优化器 ，例如从Adam换到SGD
    - 梯度爆炸或者消失（监控梯度，同时进行梯度截断）
    - 增加减小学习率
    - 神经网络梯度太深
<!-- #endregion -->
