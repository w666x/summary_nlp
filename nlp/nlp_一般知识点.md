## NLU

<!-- #region -->
### 中文分词
- Q1：中文分词都有哪些技术？
- 中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需要人为切分。
    - 根据其特点，可以把分词算法分为四大类：**基于规则的（词典）、基于统计的（比如CRF）、基于语义的、基于神经网络的**

| 分词方法 | 方法描述 | 方法特点 | 典型方法 | 优点 | 缺点
|:- |:- |:- |:- |:-  |:- 
| [基于规则的分词方法](#基于规则的分词算法) | 又称作机械分词方法、基于字典的分词方法，按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配。<br>若在词典中找到某个字符串，则匹配成功。|  该方法有三个要素，**即分词词典、文本扫描顺序和匹配原则**。 <br> 文本的扫描顺序有正向扫描、逆向扫描和双向扫描。匹配原则主要有最大匹配、最小匹配、逐词匹配和最佳匹配。| 传统的词库 | 简单，易于实现。 | 匹配速度慢；<br>存在交集型和组合型歧义切分问题；**词本身没有一个标准的定义，没有统一标准的词集**；不同词典产生的歧义也不同；缺乏自学习的智能性。
| 基于统计的分词方法 | 字与字相邻出现的概率或频率能较好地反映成词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。| 词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。| 主要的统计模型有：N 元文法模型（N-gram）、隐马尔可夫模型（Hiden Markov Model，HMM）、最大熵模型（ME）、条件随机场模型（Conditional Random Fields，CRF）等。| 在实际应用中此类分词算法一般是将其与基于词典的分词方法结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。
| 基于语义的分词方法 | 语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理 | | 如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。  |  | 由于规则难以穷尽，该方法在实践中应用不是特别多。
| 基于神经网络的分词方法 | 可以应用有关词、句子等的句法和语义信息来对分词歧义进行综合判断，它将分词知识所分散隐式的方法存入神经网络内部，通过自学习和训练修改内部权值，以达到正确的分词结果，最后给出神经网络自动分词结果 | 如使用 LSTM、GRU 等神经网络模型等 | 分词较准确，不同上下文下分词结果可能不一样



#### 分词工具介绍
- 几个比较实用的分词工具：
    - 中科院计算所[NLPIR](http://ictclas.nlpir.org/nlpir/) 
    - [ansj分词器](https://github.com/NLPchina/ansj_seg) 
    - 哈工大的[LTP](https://github.com/HIT-SCIR/ltp) 
    - 清华大学[THULAC](https://github.com/thunlp/THULAC) 
    - 斯坦福分词器，Java实现的[CRF算法](https://nlp.stanford.edu/software/segmenter.shtml) 
    - **[Hanlp分词器](https://github.com/hankcs/HanLP)，求解的是最短路径**。 
    - **结巴分词**，[jieba](https://github.com/yanyiwu/cppjieba)
        - 基于前缀词典，生成句子中所有可能成词所构成的有向无环图 (DAG)，采用动态规划查找最大概率路径, 找出基于词频的最大切分组合，
        - 对于未登录词，采用了 HMM 模型，使用 Viterbi 算法。 
    - [KCWS分词器](https://github.com/koth/kcws)(字嵌入+Bi-LSTM+CRF)。 
    - [ZPar](https://github.com/frcchang/zpar/releases)
    - [IKAnalyzer](https://github.com/wks/ik-analyzer)
<!-- #endregion -->

#### 基于规则的分词算法
- 最大匹配分词算法
    - 前向最大匹配（forward-max matching）
        - 定义最大长度max_len
        - **从首字母开始，取max_len长度的字符**，看是否在词库中有对应，没有则依次减少字符数
        - 如果有匹配，则将对应的字符抽取出来

    - 后向最大匹配
        - 定义最大长度max_len
        - **从末字母开始，取max_len长度的字符**，看是否在词库中有对应，没有则依次减少字符数
        - 如果有匹配，则将对应的字符抽取出来
        
    - 优缺点&cross;
        - 贪心算法，结果为局部最优解而不是全局最优解
        - 可能会出现oob (out of vocabulary)的情况
        - 没考虑单词之间的关系和上下文

<!-- #region -->
### 文本处理

1. 词形还原（lemmatization）是指什么？
    - 词形还原（lemmatization），即把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。
    - 主要采用“转变”的方法，将词转变为其原形，如将“drove”处理为“drive”，将“driving”处理为“drive”。
    - 词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达。

 
2. 词干提取（stemming）是指什么？
    - 词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。
    - 在原理上，词干提取主要是采用“缩减”的方法，将词转换为词干，如将“cats”处理为“cat”，将“effective”处理为“effect”。
    - **词干提取更多被应用于信息检索领域**，如Solr、Lucene等，用于扩展检索，粒度较粗。

 

3. 什么是编辑距离？如果计算两个字符串之间的编辑距离，说说大体思路？
    - 针对二个字符串（例如英文字）的差异程度的量化量测，**测量方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串**。
    - 在莱文斯坦距离中，可以删除、加入、取代字符串中的任何一个字符，也是较常用的编辑距离定义，常常提到编辑距离时，指的就是莱文斯坦距离。
<!-- #endregion -->

<!-- #region -->
4. 如何进行英文文本non-word纠错？
    - 找出错误单词；
    - 查找相近词（编辑距离近的）作为候选纠正项；
        - 利用贝叶斯定理求解最优解:
        - $P(A|B)P(B) = P(A|B) K = p(A)*p(B|A)$ 其中A表示候选项，B表示错误项，P(B)可以有经验得到，为常数
        - 即P(A|B) 正比于，“候选项本身在语料中出现的可能性”和“人们意图打候选项时会错打成错误单词的可能性”的乘积。
        - “候选项本身在语料中出现的可能性”可以扩展至n-gram，即计算与周边词一起出现的可能性。

 
5. 语料中的词频分布有什么重要规律？
    - 齐夫定律：在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。所以，频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍。
    - 两大规律：**语料中的大部分由高频词构成；词典中的大部分单词都是低频词。**

 

6. textrank原理中，以什么标准来衡量词汇的重要性？
    - 如果一个单词被很多其他单词链接到，说明这个单词比较重要；
    - 如果一个重要性很高的单词链接到一个其他单词，那么被链接到的单词的重要性会相应地因此而提高。

 

7. 什么是语言模型？
    - 语言模型本质上是在回答一个问题：**出现的语句是否合理，是否像人话**。
    - 在历史的发展中，语言模型经历了专家语法规则模型（至80年代），统计语言模型（至00年），神经网络语言模型（到目前）。

| 模型 | 模型说明 | 模型优缺点 | 是否常用
|:- |:- |:- |:-
| 专家语法规则模型 | 在计算机初始阶段，随着计算机编程语言的发展，归纳出的针对自然语言的语法规则。| $\color{blue}{缺点}$：自然语言本身的多样性、口语化，在时间、空间上的演化，及人本身强大的纠错能力，导致语法规则急剧膨胀，不可持续。 | 否
| 统计语言模型 | 统计语言模型就是计算一个句子的概率大小的这种模型。常用统计语言模型，包括了N元文法模型（N-gram Model）统计语言模型把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。| | 否
| 神经网络语言模型 | NNLM，2003年 Bengio 提出，神经网络语言模型(neural network language model, NNLM)的思想是提出词向量的概念，代替 ngram 使用离散变量(高维)，采用连续变量(具有一定维度的实数向量)来进行单词的分布式表示| $\color{red}{优点}$：解决了维度爆炸的问题，同时通过词向量可获取词之间的相似性。<br> $\color{blue}{缺点}$：因为这是通过前馈神经网络来训练语言模型，缺点显而易见就是其中的参数过多计算量较大，同时softmax那部分计算量也过大。另一方面NNLM直观上看就是使用神经网络编码的 n-gram 模型，也无法解决长期依赖的问题。 | 是
| 神经网络语言模型 | RNNLM，它是通过RNN及其变种网络来训练语言模型，任务是通过上文来预测下一个词 | $\color{red}{优点}$：它相比于NNLM的优势在于所使用的为RNN，RNN在处理序列数据方面具有天然优势，RNN 网络打破了上下文窗口的限制，使用隐藏层的状态概括历史全部语境信息，对比 NNLM 可以捕获更长的依赖，在实验中取得了更好的效果。RNNLM 超参数少，通用性更强； $\color{blue}{缺点}$：但由于 RNN 存在梯度弥散问题，使得其很难捕获更长距离的依赖信息。 | 否
| Word2vec | CBOW 以及skip-gram，其中CBOW是通过窗口大小内的上下文预测中心词，而skip-gram恰恰相反，是通过输入的中心词预测窗口大小内的上下文。| | 是
| Glove  | 属于统计语言模型，通过统计学知识来训练词向量。
| ELMO | 通过使用多层双向的LSTM（一般都是使用两层）来训练语言模型，任务是利用上下文来预测当前词，上文信息通过正向的LSTM获得，下文信息通过反向的LSTM获得，这种双向是一种弱双向性，因此获得的不是真正的上下文信息。
| GPT | 是通过Transformer Decoder来训练语言模型，它所训练的语言模型是单向的，通过上文来预测下一个单词。
| BERT | 通过Transformer Encoder来训练MLM这种真正意义上的双向的语言模型，它所训练的语言模型是根据上下文来预测当前词。
<!-- #endregion -->

<!-- #region -->
### 依存句法分析

1. 什么是依存文法分析？
    - 依存文法是由法国语言学家L.Tesniere在其著作《结构句法基础》（1959年）中提出，对语言学的发展产生了深远的影响，特别是在计算语言学界备受推崇。
    - **依存语法通过分析语言单位内成分之间的依存关系揭示其句法结构，主张句子中核心动词是支配其它成分的中心成分，而它本身却不受其它任何成分的支配，所有受支配成分都以某种依存关系从属于支配者。**
    - 在20世纪70年代，Robinson提出依存语法中关于依存关系的四条公理，在处理中文信息的研究中，中国学者提出了依存关系的第五条公理，如下：
        - 1、一个句子中只有一个成分是独立的；
        - 2、其它成分直接依存于某一成分；
        - 3、任何一个成分都不能依存与两个或两个以上的成分；
        - 4、如果A成分直接依存于B成分，而C成分在句中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分；
        - 5、中心成分左右两面的其它成分相互不发生关系。

 

- 依存文法分析有什么应用？
    - 情感分析：根据情感词汇及其语法成分、情感词与否定词之间的句法关系，构建规则，计算句子极性。
    - 事件抽取：判断句子描述的事件，通过核心谓词和语法结构，进行判断。比如核心谓词的并列关系（COO）词、核心谓词的动宾关系（VOB）词。
<!-- #endregion -->
