## 机器学习原理

<!-- #region -->
1. 为什么一些机器学习模型需要对数据进行归一化？
    - 1）归一化后加快了梯度下降求最优解的速度；
        - 若特征之间量级差距比较大的时，当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；
        - 如果**机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。**
        - xgboost等树模型是不能进行梯度下降的，因为树模型是阶越的，不可导。树模型是通过寻找特征的最优分裂点来完成优化的。由于归一化不会改变分裂点的位置，因此**xgboost不需要进行归一化**。
    - 2）归一化有可能提高精度
        - 一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。
        - 如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖


2. 哪些模型，需要归一化？填充缺失值？处理分类型变量？
    - 首先树模型基于信息增益、信息增益率、基尼指数作为特征空间划分方法，任何特征的单调变换（不影响排序结果）均不会影响模型，
    - **树模型（包括GBDT、LightGBM）都不需要进行归一化**
    - 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。
    - 而像Adaboost、GBDT、XGBoost（gblinear）、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。

| 模型名称 | 是否需要归一化 | 是否需要处理缺失值 | 是否需要处理分类变量 | 模型解释 | 是否常用
| :- | :- | :- | :- | :- | :- 
| Adaboost | Yes | | | $\color{blue}{归一化}$：adaboost如果你使用决策树做分类器，也是不用做归一化的 <br>按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”
| GBDT | Yes | Yes | Yes | $\color{blue}{归一化}$：GBDT中的树都是回归树，不是分类树，GBDT的树是在上一颗树的基础上通过梯度下降求解最优解，归一化能收敛的更快，GBDT通过减少偏差来提高性能。 <br> $\color{blue}{one-hot}$：基分类器一般为cart树，其输入通常只支持连续型数值类型的
| XGB | No | No | Yes | $\color{blue}{归一化}$：XGBoost可以支持多种弱学习器，是否需要归一化应该取决于我们采用的弱学习器类型。一种常见的实现方式是选择决策树作为弱学习器，树模型通过遍历特征所有取值来选择划分点，是否归一化并不影响这个过程的进行；<br>$\color{blue}{缺失值}$：在训练过程中对缺失值做了针对性的处理，在构建决策树的过程中尝试把被选为分裂点的特征中有缺失值的样本分到左子树或右子树，然后评估看放那边会降低损失<br>$\color{blue}{one-hot}$：xgboost只支持数值型的特征，XGB会对所有特征按取值进行排序并加载到内存，要求特征有序。如果对离散特征直接k值编码，进入到XGB后编码结果会被当成是有序的特征处理，与事实不符 | Yes
| LGB | No | No | No | $\color{blue}{归一化}$：树模型基于信息增益、信息增益率、基尼指数作为特征空间划分方法，任何特征的单调变换（不影响排序结果）均不会影响模型 <br>$\color{blue}{缺失值}$：LightGBM都可以自动处理特征缺失值, <br> $\color{blue}{one-hot}$：直接支持类别特征
| CatBoost | | | No | $\color{blue}{one-hot}$：CatBoost是一种以对称决策树 为基学习器的GBDT框架，主要为例合理地处理类别型特
| 随机森林 | No | No | No | $\color{blue}{归一化}$：随机森林的逻辑是基于决策树模型多棵树模型，内部采用boostrap抽样训练多棵树，然后做bagging预测提高其精度，决策树模型具体实现机制以一定区别，不过一般是基于排序的划分，所以**一般不需要进行归一化处理**。<br>$\color{blue}{缺失值}$：随机森林分类器可以处理缺失值<br> $\color{blue}{one-hot}$：随机森林分类器可以用分类值建模 | Yes

<!-- #endregion -->
3. 
