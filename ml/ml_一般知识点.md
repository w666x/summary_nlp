## 机器学习原理

<!-- #region -->
1. 为什么一些机器学习模型需要对数据进行归一化？
    - 1）归一化后加快了梯度下降求最优解的速度；
        - 若特征之间量级差距比较大的时，当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；
        - 如果**机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。**
        - xgboost等树模型是不能进行梯度下降的，因为树模型是阶越的，不可导。树模型是通过寻找特征的最优分裂点来完成优化的。由于归一化不会改变分裂点的位置，因此**xgboost不需要进行归一化**。
    - 2）归一化有可能提高精度
        - 一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。
        - 如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖


2. 哪些模型，需要归一化？填充缺失值？处理分类型变量？
    - 首先树模型基于信息增益、信息增益率、基尼指数作为特征空间划分方法，任何特征的单调变换（不影响排序结果）均不会影响模型，
    - **树模型（包括GBDT、LightGBM）都不需要进行归一化**
    - 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。
    - 而像Adaboost、GBDT、XGBoost（gblinear）、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。
    - 对缺失值敏感的一些分析？
        - 树模型对缺失值的敏感度脚底，大部分时候可以在数据有缺失值时使用
        - 涉及到距离度量，缺失数据就显得比较重要，不如KNN或者SVM
        - 线性模型的代价函数，比如MSE，涉及计算预测值和真实值之间的差别，一般会对缺失值敏感


| 模型名称 | 是否需要归一化 | 是否需要处理缺失值 | 是否需要处理分类变量 | 模型解释 | 是否常用
| :- | :- | :- | :- | :- | :- 
| Adaboost | Yes | | | $\color{blue}{归一化}$：adaboost如果你使用决策树做分类器，也是不用做归一化的 <br>按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”
| GBDT | Yes | Yes | Yes | $\color{blue}{归一化}$：GBDT中的树都是回归树，不是分类树，GBDT的树是在上一颗树的基础上通过梯度下降求解最优解，归一化能收敛的更快，GBDT通过减少偏差来提高性能。 <br> $\color{blue}{one-hot}$：基分类器一般为cart树，其输入通常只支持连续型数值类型的
| XGB | No | No | Yes | $\color{blue}{归一化}$：XGBoost可以支持多种弱学习器，是否需要归一化应该取决于我们采用的弱学习器类型。一种常见的实现方式是选择决策树作为弱学习器，树模型通过遍历特征所有取值来选择划分点，是否归一化并不影响这个过程的进行；<br>$\color{blue}{缺失值}$：在训练过程中对缺失值做了针对性的处理，在构建决策树的过程中尝试把被选为分裂点的特征中有缺失值的样本分到左子树或右子树，然后评估看放那边会降低损失<br>$\color{blue}{one-hot}$：xgboost只支持数值型的特征，XGB会对所有特征按取值进行排序并加载到内存，要求特征有序。如果对离散特征直接k值编码，进入到XGB后编码结果会被当成是有序的特征处理，与事实不符 | Yes
| LGB | No | No | No | $\color{blue}{归一化}$：树模型基于信息增益、信息增益率、基尼指数作为特征空间划分方法，任何特征的单调变换（不影响排序结果）均不会影响模型 <br>$\color{blue}{缺失值}$：LightGBM都可以自动处理特征缺失值, <br> $\color{blue}{one-hot}$：直接支持类别特征
| CatBoost | | | No | $\color{blue}{one-hot}$：CatBoost是一种以对称决策树 为基学习器的GBDT框架，主要为例合理地处理类别型特
| 随机森林 | No | No | No | $\color{blue}{归一化}$：随机森林的逻辑是基于决策树模型多棵树模型，内部采用boostrap抽样训练多棵树，然后做bagging预测提高其精度，决策树模型具体实现机制以一定区别，不过一般是基于排序的划分，所以**一般不需要进行归一化处理**。<br>$\color{blue}{缺失值}$：随机森林分类器可以处理缺失值<br> $\color{blue}{one-hot}$：随机森林分类器可以用分类值建模 | Yes

<!-- #endregion -->
3. 随机梯度下降法的问题和挑战？
    - 在机器学习和统计参数估计问题中，目标函数通常是求和函数的形式，形似 $J_X(\theta) = \sum_iJ_{x_i}(\theta)$
        - 其中，每一个函数 $J_{x_i}(\theta)都对应一个样本x_i$
        - 当样本极大时，梯度的计算就会变得非常耗时，此时也就有了随机梯度下降/小批量梯度下降算法啦
    - 问题：为啥不使用牛顿法？
        - 牛顿法要求计算目标函数的2阶导数（海森矩阵），在高维特征的情况下，这个矩阵非常大，计算和存储都有问题
        - 在使用小批量的情形下，牛顿法对于二阶导数的估计噪音太大
        - 在目标函数非凸时，牛顿法更加容易收到鞍点甚至最大值点的吸引

<!-- #region -->
4. 什么是最小二乘法？
    - 最小二乘法（又称最小平方法）是一种数学优化技术。**它通过最小化误差平方和寻找最优参数**。
    - 利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。
    
    
5. 对偶的概念？
    - 解释对偶的概念。
    - 一个优化问题可以从两个角度进行考察，一个是primal问题，一个是dual问题，就是对偶问题，
    - 一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，
    -  对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将Primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。
    
    
    
6. SVD vs PCA
    - PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。
    - 而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。
<!-- #endregion -->

<!-- #region -->
7. 常用的优化方法？
    - 一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。
    - 二阶方法：牛顿法、拟牛顿法：
        - **牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。**
        - 在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。
    - 牛顿法
        - 实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。
        - 我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。
        - 缺点：**牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。**
    - 拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。
        - 拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。
        - 主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。 
       

| 方法名称 | 优点 | 缺点
| :-| :-| :-
| 随机梯度下降 | 优点：可以一定程度上解决局部最优解的问题 | 缺点：收敛速度较慢
| 批量梯度下降 | 优点：收敛速度较快 | 缺点：容易陷入局部最优解
| mini_batch梯度下降 | 综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
| 牛顿法 | 收敛速度更快啦 | 牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。
| 拟牛顿法 | 拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。
<!-- #endregion -->

<!-- #region -->
8. cart树计算gini系数说明？
    - 对于离散型特征来说，把所有离散特征用来分类，并比较一下，选用最小的，然后接下来的节点继续这样做
    - 对于连续特征，具体的思路如下，
        - 比如m个样本的连续特征A有m个，从小到大排列为 $a_1, a_2, \cdots, a_m$，
        - **CART算法取相邻两样本值的平均数，一共取得m-1个划分点**，其中第i个划分点Ti表示为：$Ti=\frac{a_i + a_{i + 1}}{2}$。
        - 对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。
        - 选择基尼系数最小的点作为分类点。对于多分类也是一样，反正都是度量基尼系数。
    - 对于缺失值的处理，主要是改变一下基尼系数的算法，把缺失值和非缺失值的样本提出来，
        - 计算（非缺失的部分的基尼系数）*（非缺失/总样本数）,以此作为基尼系数计算依据，然后分类。
        - 然后可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。
        
        
9. CART树优化方法？
    - **决策树没有什么梯度下降等优化方法，就是计算然后比较，所以这里就介绍一下它的剪枝方法。**
    - 由于决策树容易过拟合，为了增加泛化性能，我们对决策树某些太细分的节点进行剪枝，那么怎么判断是否是太过细分了呢？
    - CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略
    - 也就是说，CART树的剪枝算法可以概括为两步，
        - 第一步是从原始决策树生成各种剪枝效果的决策树
        - 第二步是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的树作为最终的CART树
<!-- #endregion -->

## 训练


1. 当机器学习性能遭遇瓶颈时，你会如何优化的？
    - 可以从这4个方面进行尝试：基于数据、借助算法、用算法调参、借助模型融合
    - 获得更多的数据：你能够拿到更多或者更高质量的数据么？对现代非线性机器学习模型如深度学习而言，数据越多，改进越多。
        - 创造更多数据：如果你不能拿到更多数据，那么，你能创造出新的数据么？也许你可以填充或者重新排列现有数据，或者利用概率模型来产生新的数据。
        - 清洁你的数据：你能否改善数据中的信号？也许可以纠正或删除一些缺失或错误的观测值，或者在合理范围外的离群点，从而提升数据质量。
        - 数据重新取样：你能否对数据重新取样，以改变其大小或者分布？也许你可以用一个小得多的数据来实验，以提高实验的速度；或对某个特殊类型的观察值进行过采样/欠采样以使得它们更好地代表整个数据集。
        - 重新界定问题：你能否改变你正试图解决的问题类型？重构数据，如回归，二项或多项分类，时间序列，异常检测，评分，推荐等问题类型。
        - 重新缩放数据：你能否对数值型变量进行缩放处理？输入数据的归一化和标准化处理可以提升使用加权或距离度量的算法性能。
        - **转化数据**：你能否改变数据的分布形态？使得数据更服从高斯分布，或**进行指数变换可能会暴露出数据更多的特征供算法学习**。
        - 数据投影(映射)：你能否将数据投影到一个更低维的空间？你可以用无监督的聚类或投影方法，创造一个新的压缩数据集代表。
        - 特征选择：所有的输入变量是否同等重要？使用特征选择和衡量特征重要性的方法，可以创造出数据的新视角，供模型算法探索。
        - 特征工程：你能够创造或者增加新的特征？也许有的属性可以分解为多个新的值（比如类别，日期或字符串）或者属性可以聚集起来代表一个事件（如一个计数，二进制标志或统计信息）
    - 借助算法改善性能
        - 重采样方法：要用什么样的重采样方法来估计其在新数据上的能力？使用一种能够最好地利用现有数据的方法和参数设置。K折交叉验证法，利用其中的一折作为验证集可能是最佳操作。
        - 评价指标：**用什么样的指标来评价预测能力？选择能够最好地体现问题和专业需求的指标。不要任何问题一上来就看分类准确率。**
        - 基线性能：比较算法时，什么是基线性能？通过随机算法或零规则算法（预测均值或众数）来建立一个基线，并以此对所有算法进行排序。
        - 抽检线性算法：什么样的线性算法能有好结果？线性方法通常更容易产生偏倚，也易于理解，能快速训练。如果能达到好效果，则更容易被选中。评估多个不同的线性方法。
        - 抽检非线性算法：哪些非线性算法能有好结果？非线性算法通常要求更多数据，有更高的复杂性，但是能获得更好的性能。评估多个不同的非线性方法。
        - 从文献中偷师学艺：哪些文献报导的方法能很好地解决你的问题？也许你能从算法类型或传统方法的延伸中获取解决自己问题的灵感。
        - 标准参数设置：评估算法时，什么是标准的参数设置？每一个算法都有机会解决你的问题，这不是说在现有基础上死磕调参，而是说，每一种算法都需要把参数调好，才能在算法“大赛”中有胜出的机会。
    - 通过算法调参改善性能
        - 诊断：对算法要做哪些诊断和回顾？也许可以回顾一下学习曲线，**了解目前模型的状态是过拟合还是欠拟合，然后纠正它**。不同的算法可能提供不同的可视化结果和诊断。检视算法得到正确预测结果和错误预测结果的样本。
        - 随机搜索：哪些参数可以用随机搜索？也许你可使用算法超参数的随机搜索，来发现那些你永远也想不到的参数设置。
        - 网格搜索：哪些参数可以使用网格搜索？也许有一些标准超参数网格值，你可以拿来赋值，从而发现好的参数设置，重复这一过程，不断精调网格。
    - 借助模型融合改善性能
        - 混合模型预测结果：你是否可以直接组合多个模型的预测结果？也许你可以使用同样的或不同的算法来搭建多个模型。对各自的预测结果取均值，或者众数。
        - 混合数据呈现方式：你是否可以组合用不同数据呈现方法得到的模型预测结果？也许你使用了不同的问题投射方法，来训练性能良好的的算法，那么这些预测结果可以组合起来。
        - 混合数据样本：你是否可以组合不同数据角度(特征)训练的模型？也许你可以创造训练样本的多个子样本来训练一个性能良好的算法，然后把结果组合起来。这叫做自助聚集（bootstrap aggregation）或者bagging，当各个模型的预测都很高明而方法各异（不相关）时，效果最好。
        - 纠正预测。你是否可以纠正性能良好模型的预测？也许你可以明确地纠正预测结果，或者通过像boosting这样的方法来学习如何纠正预测错误。


<!-- #region -->
2. 如何进行特征选择？
    - 特征选择是一个重要的数据预处理过程，主要有两个原因：
        - 一是**减少特征数量、降维，使模型泛化能力更强，减少过拟合;**
        - 二是增强对特征和特征值之间的理解。
        
        
3. 常见的特征选择方式：
    - 1.去除方差较小的特征。
    - 2.正则化。L1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
    - 3.随机森林，**对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。**
        - 一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，
        - 1是重要的特征有可能得分很低（关联特征问题或者叫做共线性），
        - **2是这种方法对特征变量类别多的特征越有利（偏向问题）。**
    - 4.稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。
        - 它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，
        - 比如可以**统计某个特征被认为是重要特征的频率**（被选为重要特征的次数除以它所在的子集被测试的次数）。
        - 理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。
        
        
4. 数据预处理的方式？
    - 1.缺失值，填充缺失值fillna：
        - i. 离散：None,
        - ii. 连续：均值。
        - iii. 缺失值太多，则直接去除该列
    - 2.连续值：离散化。有的模型（如决策树）需要离散值
    - 3.对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作
    - 4.**皮尔逊相关系数，去除高度相关的列**
    
    
5. 数据不平衡问题处理？
    - 1）**采样，对小样本加噪声采样，对大样本进行下采样**
    - 2）进行特殊的加权，如在Adaboost中或者SVM中
    - 3）采用对不平衡数据集不敏感的算法
    - 4）**改变评价标准：用AUC/ROC来进行评价**
    - 5）采用Bagging/Boosting/Ensemble等方法
    - 6）考虑数据的先验分布
<!-- #endregion -->
