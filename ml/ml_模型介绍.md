<!-- #region -->
## 模型介绍

| 任务 | 算法demo
| :- |:-
| 分类任务 | SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯
| 有监督学习 | 感知机、SVM、人工神经网络、决策树、逻辑回归
| 判别模型 | 逻辑回归、支持向量机、CRF、线性回归
| 生成模型 | HMM、朴素贝叶斯、LDA


| 算法名称 | 定义 | 特点 | 输入 | 输出 | 优点 | 缺点 | 损失函数 | 目标函数 | 是否常用
| :-| :-| :-| :-| :-| :-| :-| :-| :-| :-
| 决策树-ID3 | 通过信息论中的信息增益来寻找最优的节点，信息增益最大，就选哪个特征 | ID3的分裂不是2分裂的方式 |  | | | 无法处理缺失值，无法处理连续特征，且如果一个特征分类多，天然占优势。还有过拟合的问题 |  信息增益
| 决策树-C4.5 | | 分裂方式还是多叉树 |  | | 对于ID3不能处理连续特征：把连续特征离散化，选出每个连续特征的中间节点。<br>对于特征分类多天然占优的情况：<br>改进一下用信息增益比来度量（连续特征还是信息增益）。 <br> 过拟合与CART树采用相同的剪枝处理。对缺失值处理：用CART树相同的方法。 | C4.5也有缺点，计算用的熵模型，费时间。<br> **只能用于分类。** | 用信息增益比来度量（连续特征还是信息增益）
| 决策树-CART | 决策树是一个经典的分类算法，用if,else构成，可以分类，回归，也可以拿来做集成模型的弱学习器 | 2分裂  | 样本X，属于的分类标签y | 回归：cart回归树是用叶子结点的均值，来作为回归的输出值| | | 分类问题：基尼系数，我们希望每次选节点时，选取可以让基尼系数最小的节点<br> 回归问题：和方差度量损失，选择分出的两个集合的和方差最小的来进行节点分组
| 随机森林 | 集成模型中的bagging模型，其基本思想是训练多棵树对同一数据进行预测，然后把这些树预测结果做一个投票，投票加总进行分类 | 随机采样：这里每棵树的训练在样本中有放回随机采样，一般采m个。<br>训练过程：随机森林的每棵树训练过程与cart有一点不同，他不会选最优节点来划分，而是先随机选一个特征子集，再在子集中选最优，增强泛化能力。<br>何处理缺失值呢？和CART树一样，把缺失的特征的基尼系数\*权重（意思就是你缺失了，你就乘以0.x，相比不缺失的特征，你就往后稍一稍），实在要用你分类了，缺失特征值的样本同时划入你分的两类中，同时这些样本要分别乘以权重（这里权重就是分开的比例） | | | | |
| 梯度提升树GBDT-回归数 | 集成学习中的Boosting方法，而boosting方法是前面的学习结果要影响后面的一种方法（比如：这个弱学习器预测出来的一个样本的结果不如意，下一个弱学习器训练时，这个样本的权重就会增加，这样算损失函数的时候就会偏向这个样本多一些） | 用“损失函数的负梯度”来拟合本轮损失近似值<br>  | | | | | 回归问题：均方差、绝对误差、huber损失：是均方差和绝对损失的折合，超过了一定距离的异常点就用绝对损失。<br> 分类问题：指数损失函数，和对数损失函数
| 梯度提升树GBDT-分类树 | 基本思想是和逻辑回归相似，用概率来表示分类，极大似然概率来测量损失 | | | | | | 二分类问题：L(y,f(x)) = log(1 + exp(-yf(x)))，负的对数似然函数做损失函数
| xgboost | | 1.用二阶泰勒展开来做损失，速度更快且支持自定义损失函数 <br>2.树最后的回归值选择（ctj）选择跟GBDT不同，分类节点选择跟cart树也不同，对单颗树的分裂过程做了并行优化<br>3.缺失值的处理：对于缺失值的特征，通过比较所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。 |||||XGB的损失函数，加入了正则化，叶子节点的个数及叶子节点的最优值<br> **XGBoost这里不使用GBDT中的均方误差，而是使用贪心法，即每次分裂都期望向着最小化损失函数的地方前进，即最大化第一层的损失函数减去第二层的损失函数** |
| LightGBM | | 1.把特征值弄成直方图，减小节点数量，通过直方图来做节点判断，这样计算量小了许多。<br>2.树的分裂过程，不是所有节点同时二分(按层生长)，而是一次只选择一个节点二分（leaf-wise）。<br>3.**可直接使用category特征，对分类特征不用再进行one-hot编码了**(one-hot对于类别很多的特征来说没有价值)
<!-- #endregion -->

### 线性回归模型

<!-- #region -->
#### 模型优缺点
- 优点：
    - 善于获取数据集中的线性关系；
    - 训练速度和预测速度较快；
    - 在小数据集上表现很好；
    - 结果可解释，并且易于说明；


- 缺点：
    - 不适用于非线性数据；
    - 预测精确度较低；
    - 可能会出现过拟合（下面的正则化模型可以抵消这个影响， Lasso回归, Ridge回归, Elastic-Net回归）；
    - 分离信号和噪声的效果不理想，在使用前需要去掉不相关的特征。
<!-- #endregion -->

### 逻辑回归

<!-- #region -->
#### 相关内容

- 定义
    - **Logistic回归目的是从特征学习出一个0/1分类模型**，而这个模型是将特征的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。
    - **Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小**
    - 因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。
    - 如下，x是n为特征向量，函数g为Logistic函数

$$h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}} \\ P(y=1|x;\theta) = h_\theta(x) \\ P(y=0|x;\theta) = 1 - h_\theta(x)$$




- 逻辑回归为啥不用mse
    - 因为用MSE作为二元分类的损失函数会有梯度消失的问题
    - logistics回归的损失函数的梯度求解，如下图所示
    
![logistics回归的损失函数](https://cdn.jsdelivr.net/gh/w666x/image/NLP_base/logistics回归的损失函数.jpg)
<!-- #endregion -->

### SVM

| 支持向量机分类 | 定义 | 特点
|:- |:- |:-
| 线性可分支持向量机 | 训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器 | 训练样本线性可分
| 线性支持向量机 | 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器 | 当训练样本近似线性可分\
| 非线性支持向量机 | 当训练样本线性不可分时，通过使用核函数和软间隔最大化 | 当训练样本线性不可分


<!-- #region -->
#### 模型介绍
- SVM是一种二分类模型。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。
    - 它的基本思想是在**特征空间中寻找间隔最大化的分离超平面使得数据得到高效的二分类**，
    - 具体来讲，有三种情况
        - 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
        - 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
        - 当训练样本线性不可分时，通过使用核函数和软间隔最大化，学习一个非线性支持向量机。
            - 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

- 核函数类别
    - 核函数对应某一变换空间的内积。
    - SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。
    - 在选取核函数解决实际问题时，通常采用的方法有：
        - 一是利用专家的先验知识预先选定核函数；
        - 二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．
            - 如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多。
        - 三是采用由Smits等人提出的**混合核函数方法，该方法较之前两者是目前选取核函数的主流方法**，
            - 也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想。


- 如何选择核函数
    - 当特征维数d超过样本数m时(文本分类问题通常是这种情况),使用线性核;
    - 当特征维数d比较小.样本数m中等时,使用RBF核;
    - 当特征维数d比较小.样本数m特别大时,支持向量机性能通常不如深度神经网络，所以，上神经网络吧
<!-- #endregion -->

<!-- #region -->
#### SVM如何用于多分类
- 一对多法
    - **训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。**
    - 分类时将未知样本分类为具有最大分类函数值的那类。


- demo
    - 假如我有四类要划分（也就是4个Label），他们是A、B、C、D。于是我在抽取训练集的时候，分别抽取
        - （1）A所对应的向量作为正集，B，C，D所对应的向量作为负集；
        - （2）B所对应的向量作为正集，A，C，D所对应的向量作为负集；
        - （3）C所对应的向量作为正集，A，B，D所对应的向量作为负集；
        - （4）D所对应的向量作为正集，A，B，C所对应的向量作为负集；
    - 使用这四个训练集分别进行训练，然后的得到四个训练结果文件。在测试的时候，把对应的测试向量分别利用这四个训练结果文件进行测试。
    - 最后每个测试都有一个结果f1(x),f2(x),f3(x),f4(x)。
    - **于是最终的结果便是这四个值中最大的一个作为分类结果**。
    
    
- 一对一法（one-versus-one,简称OVO SVMs或者pairwise）
    - 其做法是在任意两类样本之间设计一个SVM，因此k个类别的样本就需要设计k(k-1)/2个SVM。
    - 当对一个未知样本进行分类时，最后得票最多的类别即为该未知样本的类别。


- 层次支持向量机
    - 首先将所有类别划分成两个子类，
    - 再将子类进一步划分成两个次级子类，如此循环下去，直到得到一个单独的子类为止，这样就得到一个倒立的二叉分类树。
<!-- #endregion -->

### xgboost模型


<!-- #region -->
1. GBoost怎么给特征评分
    - 在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。


2. xgboost的优点
    - 1）GBDT以传统CART作为基分类器，而XGBoost支持线性分类器，相当于引入L1和L2正则化项的逻辑回归（分类问题）和<br>
    线性回归（回归问题）；
    - 2）GBDT在优化时只用到一阶导数，XGBoost对代价函数做了二阶Talor展开，引入了一阶导数和二阶导数。**XGBoost支持自定义的损失函数，只要是能满足二阶连续可导的函数均可以作为损失函数；**
    - 3）XGBoost在损失函数中引入正则化项，用于控制模型的复杂度。正则化项包含全部叶子节点的个数，每个叶子节点输出的score的L2模的平方和。从Bias-variance tradeoff角度考虑，正则项降低了模型的方差，防止模型过拟合，这也是xgboost优于传统GBDT的一个特性。
    - 4）当样本存在缺失值是，xgBoosting能自动学习分裂方向，**即XGBoost对样本缺失值不敏感**；
    - 5）XGBoost借鉴RF的做法，支持列抽样，这样不仅能防止过拟合，还能降低计算，这也是xgboost异于传统gbdt的一个特性。
    - 6）XGBoost在每次迭代之后，会将叶子节点的权重乘上一个学习率（相当于XGBoost中的eta，论文中的Shrinkage），主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点；
    - 7）**XGBoost工具支持并行，但并行不是tree粒度的并行，** XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值），XGBoost的并行是在特征粒度上的。XGBoost在训练之前，预先对数据进行了排序，然后保存为(block)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个块结构也使得并行成为了可能，**在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行；**
    - 8）可并行的近似直方图算法，树结点在进行分裂时，需要计算每个节点的增益，若数据量较大，对所有节点的特征进行排序，遍历的得到最优分割点，这种贪心法异常耗时，这时引进近似直方图算法，用于生成高效的分割点，即用分裂后的某种值减去分裂前的某种值，获得增益，为了限制树的增长，引入阈值，当增益大于阈值时，进行分裂；
    - 9) XGBoost的原生语言为C/C++，这是也是它训练速度快的一个原因。
    
    
3. 为啥xgboost应用二阶泰勒展开？
    - 比一阶泰勒展开，即梯度下降法，收敛速度更快
    - 二阶泰勒展开，即XGBoost支持自定义的损失函数，只要是能满足二阶连续可导的函数均可以作为损失函数
    - 1）xgb之所以使用二阶梯度信息，是因为从泰勒展开式来看，gbdt使用的一阶梯度的泰勒展开式，丢失了很多的信息，使用二阶可以使损失函数更加准确。从泰勒展开的角度来看展开的次数越多越能更精准的表示损失函数的值，
    - 2）但是如果我们使用二阶梯度就要要求损失函数二阶可导，如果使用n阶展开就要求损失函数n阶可导，但是有很多损失函数不是n阶可导的，比如均方误差，因此使用二阶梯度信息是一个泰勒展开和损失函数选择的折中。
    
    
4. XGBoost如何寻找最优特征？是又放回还是无放回的呢？
    - XGBoost在训练的过程中给出各个特征的评分，即GiNI增益，从而表明每个特征对模型训练的重要性.。
    - XGBoost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴)。但XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。
<!-- #endregion -->

<!-- #region -->
### gbdt

- 基分类器是CART(classification and regression tree),使用gini系数作为指标：
- GBDT是一种基于boosting集成思想的加法模型，**训练时采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。**


- gbdt vs adaboost
    - **将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。**
    - 和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。
    - 但是对于一般损失函数怎么计算呢？
        - GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，
        - 方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。 


1. 为什么叫Gradient Boosting？
    - 因为上面提到的这个残差，**其实就是样本对于当前已有分类器的梯度**，
    - 我们再往后的学习的y是这个残差，梯度的计算取决于损失函数，如果是经典L2损失，残差就是2*(yi'-yi),



2. GBDT为什么拟合负梯度而不是残差?
    - 函数中下降最快的方向是导数方向，同理：GBDT中，损失函数减小最快的方向也是本身的导数方向。
    - 当损失函数为均方误差时，损失函数的负梯度和残差一样，但当损失函数不为均方误差时，本质上拟合的还是负梯度。
<!-- #endregion -->

### adaboost

- 1. Adaboost，权值更新公式。
    - 当弱分类器是Gm时，每个样本的的权重是w1，w2…，请写出最终的决策公式。
    - step1：基本分类器为： $G_m(x): x \leftarrow {-1, 1}$
    - step2：Gm(x)在训练数据集上的分类错误率： $e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)$
        - Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。
    - step3：Gm(x)的系数， $\alpha_m 表示G_m(x)$在最终分类器中的重要程度， $\alpha_m  = \frac{1}{2}\log\frac{1-e_m}{e_m}$
        - 有 $e_m \lt \frac{1}{2}时，\alpha_m \le 0，且\alpha_m 随着e_m$的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大
    - step4：更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代
        - 使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。
        - 就这样，通过这样的方式，AdaBoost方法能“聚焦于”那些较难分的样本上
    $$D_{m + 1} = (w_{m+1,1}, w_{m+1,2}, w_{m+1,3}, \dots, w_{m+1,i}, \cdots, w_{m+1, N})$$
    $$w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))), i = 1,2,\cdots, N$$
    $$规范因子，Z_m = \sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i)))$$
    - step5：组合各个弱分类器， $f(x) = \sum_{m=1}^M\alpha_mG_m(x)$
    - step6：得到最终的分类器， $\displaystyle G(x) = sign(f(x)) = sign(\sum_{m=1}^M\alpha_mG_m(x))$


### 贝叶斯


#### 朴素贝叶斯
- 1. 为什么朴素贝叶斯如此“朴素”？
    - 因为它假定**所有的特征在数据集中的作用是同样重要和独立的。**
    - 正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

<!-- #region -->
### k-means

- KMeans初始类簇中心点的选取？
    - K-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。
    - 1.从输入的数据点集合中随机选择一个点作为第一个聚类中心
    - 2.对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
    - 3.选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
    - 4.重复2和3直到k个聚类中心被选出来
    - 5.利用这k个初始的聚类中心来运行标准的k-means算法


- 复杂度定义
    - Kmeans是聚类方法，典型的无监督学习方法
    - 时间复杂度：O(tkmn)，其中，t为迭代次数，k为簇的数目，m为记录数，n为维数
    - 空间复杂度：O((m+k)n)，其中，K为簇的数目，m为记录数，n为维数。
    

    
- Kmeans如何优化？
    - 将所有的观测实例构建成一颗kd树，
    - 之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。
<!-- #endregion -->

### 聚类方法
- 分类
    - 1.基于划分的聚类:K-means，k-medoids，CLARANS。
    - 2.基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。
    - 3.基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。
    - 4.基于网格的方法：STING，WaveCluster。
    - 5.基于模型的聚类：EM,SOM，COBWEB。


#### KNN算法
- 定义
    - 1.计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
    - 2.对上面所有的距离值进行排序；
    - 3.选前k个最小距离的样本；
    - 4.根据这k个样本的标签进行投票，得到最后的分类类别；

<!-- #region -->
### 随机森林
- 随机森林定义
    - 随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：
        - 1）Boostrap从袋内有放回的抽取样本值
        - 2）每次随机抽取一定数量的特征（通常为sqr(n)。
    - 分类问题：采用Bagging投票的方式选择类别频次最高的
    - 回归问题：直接取每颗树结果的平均值。



1. 随机森林如何处理缺失值？
    - 方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。
    - 方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。
        - 先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，然后迭代4-6次，这个补缺失值的思想和KNN有些类似，再回头看缺失值，
        - 如果是分类变量，缺失的观测实例的相似度矩阵中的权重进行投票。
        - 如果是连续型变量，则用相似度矩阵进行加权平均的方法补缺失值。
        
        
        
2. 随机森林如何评估特征重要性？
    - 衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy
    
| 方法名称 | 定义 | 使用场景
| :- |:- |:- 
| Decrease GINI | GINI系数降低 | 对于回归问题，直接使用argmax(VarVarLeft, VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。
| Decrease Accuracy | 准确率降低 | 对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。**基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。**
<!-- #endregion -->

## 模型对比

<!-- #region -->
### 集成学习

| 方法类别 | 代表 | 步骤
|:- |:- |:-
| Bagging | 随机森林、dropout | s1：利用自助采样法对训练集随机采样，重复进行 T 次; <br> s2：基于每个采样集训练一个基学习器，并得到 T 个基学习器； <br> s3：预测时，集体**投票决策** 。
| Boosting | adaboost，gbdt  | s1：先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；<br> s2：根据上一个基学习器的表现，调整样本权重，使分类错误的样本得到更多的关注；<br> s3：基于调整后的样本分布，训练下一个基学习器； <br> s4：测试时，对各基学习器加权得到最终结果；
| stacking | 训练一个汇总模型来对子分类器结果进行汇总, 子分类器之间没有依赖关系。



- 为什么基分类器是决策树，因为它不稳定，所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。
    - 决策树的划分准则：信息增益（划分后熵越小，信息增益越大）、信息增益比、gini系数（基尼不纯度，表示集合的不确定性）
<!-- #endregion -->

### rf vs gbdt
- 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
- 1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。
- 2）不同点：
```
    组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
    组成随机森林的树可以并行生成，而GBDT是串行生成
    随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
    随机森林对异常值不敏感，而GBDT对异常值比较敏感
    随机森林是减少模型的方差，而GBDT是减少模型的偏差
    随机森林不需要进行特征归一化，而GBDT则需要进行特征归一化
```


### xgboost vs GBDT
- 1. GBDT是机器学习算法，XGBoost是该算法的一种工程实现
- 2. XGBoost在使用CART作为基学习器时，加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力
- 3. GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，可以同时使用一阶和二阶导数
- 4. XGBoost支持自定义损失函数，增强了模型的扩展性
- 5. 传统的GBDT采用CART作为基学习器（也叫基分类器），
    - XGBoost支持多种类型的基学习器，包括树模型（gbtree和dart，dart为一种引入dropout的树模型）和线性模型（gblinear），默认为gbtree
- 6. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost支持对数据进列采样，即特征采样，有利于防止过拟合，同时可以减少计算量，提高训练的效率
- 7. 传统的GBDT不能支持缺失值的处理（必须填充），**XGBoost支持缺失值的处理，能够自动学习出缺失值的分裂方向（无需填充）**
- 8. 节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的。


### XGboost vs LightGBM

- 1）寻找分割点时，差异大
    - XGBoost采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，
    - LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低，但是不能找到最精确的数据分割点；
- 2）生长策略不同
    - XGBoost采用level-wise生成决策树策略，同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合，但很多叶子节点的分裂增益较低，没必要进行更进一步的分裂，这就带来了不必要的开销；
    - LightGBM采用leaf-wise生长策略，每次从当前叶子中选择增益最大的叶子进行分裂，如此循环，但会生长出更深的决策树，产生过拟合，因此 LightGBM 在leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合）。另一个比较巧妙的优化是 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。

<!-- #region -->
### lightgbm vs xgboost
- 模型精度：XGBoost和LightGBM相当。
- 训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)
- 内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)
- 缺失值特征：**XGBoost和LightGBM都可以自动处理特征缺失值**。
- 分类特征：**XGBoost不支持类别特征**，需要OneHot编码预处理。LightGBM直接支持类别特征。


- LightGBM在XGBoost上主要有3方面的优化。
    - 1、Histogram算法:直方图算法。
    - 2、GOSS算法:基于梯度的单边采样算法。
    - 3、EFB算法:互斥特征捆绑算法。
<!-- #endregion -->

<!-- #region -->
### 逻辑回归 vs 支持向量机

1. LR和SVM的联系？
    - 1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
    - 2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。
    - 3、两者都是线性模型
    
    
    
2. LR和SVM的区别？
    - 1、LR是参数模型，SVM是非参数模型。
    - 2、从目标函数来看，**区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss**.
        - 这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
    - 3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。**而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。**
    - 4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
    - 5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。
<!-- #endregion -->

### 逻辑回归 vs 线性回归

1. 逻辑回归与线性回归的区别和联系？
    - 个人感觉逻辑回归和线性回归首先都是广义的线性回归，
    - 其次**经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，**
    - 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
    - 逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

```python

```
