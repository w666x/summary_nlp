## 模型介绍


### 线性回归模型

<!-- #region -->
#### 模型优缺点
- 优点：
    - 善于获取数据集中的线性关系；
    - 训练速度和预测速度较快；
    - 在小数据集上表现很好；
    - 结果可解释，并且易于说明；


- 缺点：
    - 不适用于非线性数据；
    - 预测精确度较低；
    - 可能会出现过拟合（下面的正则化模型可以抵消这个影响， Lasso回归, Ridge回归, Elastic-Net回归）；
    - 分离信号和噪声的效果不理想，在使用前需要去掉不相关的特征。
<!-- #endregion -->

### 逻辑回归


#### 相关内容
- 逻辑回归为啥不用mse
    - 因为用MSE作为二元分类的损失函数会有梯度消失的问题
    - logistics回归的损失函数的梯度求解，如下图所示
    
![logistics回归的损失函数](https://cdn.jsdelivr.net/gh/w666x/image/NLP_base/logistics回归的损失函数.jpg)


### SVM

| 支持向量机分类 | 定义 | 特点
|:- |:- |:-
| 线性可分支持向量机 | 训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器 | 训练样本线性可分
| 线性支持向量机 | 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器 | 当训练样本近似线性可分\
| 非线性支持向量机 | 当训练样本线性不可分时，通过使用核函数和软间隔最大化 | 当训练样本线性不可分


<!-- #region -->
#### 模型介绍
- SVM是一种二分类模型。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。
    - 它的基本思想是在**特征空间中寻找间隔最大化的分离超平面使得数据得到高效的二分类**，
    - 具体来讲，有三种情况
        - 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
        - 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
        - 当训练样本线性不可分时，通过使用核函数和软间隔最大化，学习一个非线性支持向量机。
            - 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。


- 如何选择核函数
    - 当特征维数d超过样本数m时(文本分类问题通常是这种情况),使用线性核;
    - 当特征维数d比较小.样本数m中等时,使用RBF核;
    - 当特征维数d比较小.样本数m特别大时,支持向量机性能通常不如深度神经网络，所以，上神经网络吧
<!-- #endregion -->

<!-- #region -->
#### SVM如何用于多分类
- 一对多法
    - **训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。**
    - 分类时将未知样本分类为具有最大分类函数值的那类。


- demo
    - 假如我有四类要划分（也就是4个Label），他们是A、B、C、D。于是我在抽取训练集的时候，分别抽取
        - （1）A所对应的向量作为正集，B，C，D所对应的向量作为负集；
        - （2）B所对应的向量作为正集，A，C，D所对应的向量作为负集；
        - （3）C所对应的向量作为正集，A，B，D所对应的向量作为负集；
        - （4）D所对应的向量作为正集，A，B，C所对应的向量作为负集；
    - 使用这四个训练集分别进行训练，然后的得到四个训练结果文件。在测试的时候，把对应的测试向量分别利用这四个训练结果文件进行测试。
    - 最后每个测试都有一个结果f1(x),f2(x),f3(x),f4(x)。
    - **于是最终的结果便是这四个值中最大的一个作为分类结果**。
    
    
- 一对一法（one-versus-one,简称OVO SVMs或者pairwise）
    - 其做法是在任意两类样本之间设计一个SVM，因此k个类别的样本就需要设计k(k-1)/2个SVM。
    - 当对一个未知样本进行分类时，最后得票最多的类别即为该未知样本的类别。


- 层次支持向量机
    - 首先将所有类别划分成两个子类，
    - 再将子类进一步划分成两个次级子类，如此循环下去，直到得到一个单独的子类为止，这样就得到一个倒立的二叉分类树。
<!-- #endregion -->

### xgboost模型


<!-- #region -->
1. xgboost



2. xgboost的优点
    - 1）GBDT以传统CART作为基分类器，而XGBoost支持线性分类器，相当于引入L1和L2正则化项的逻辑回归（分类问题）和<br>
    线性回归（回归问题）；
    - 2）GBDT在优化时只用到一阶导数，XGBoost对代价函数做了二阶Talor展开，引入了一阶导数和二阶导数。**XGBoost支持自定义的损失函数，只要是能满足二阶连续可导的函数均可以作为损失函数；**
    - 3）XGBoost在损失函数中引入正则化项，用于控制模型的复杂度。正则化项包含全部叶子节点的个数，每个叶子节点输出的score的L2模的平方和。从Bias-variance tradeoff角度考虑，正则项降低了模型的方差，防止模型过拟合，这也是xgboost优于传统GBDT的一个特性。
    - 4）当样本存在缺失值是，xgBoosting能自动学习分裂方向，**即XGBoost对样本缺失值不敏感**；
    - 5）XGBoost借鉴RF的做法，支持列抽样，这样不仅能防止过拟合，还能降低计算，这也是xgboost异于传统gbdt的一个特性。
    - 6）XGBoost在每次迭代之后，会将叶子节点的权重乘上一个学习率（相当于XGBoost中的eta，论文中的Shrinkage），主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点；
    - 7）**XGBoost工具支持并行，但并行不是tree粒度的并行，** XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值），XGBoost的并行是在特征粒度上的。XGBoost在训练之前，预先对数据进行了排序，然后保存为(block)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个块结构也使得并行成为了可能，**在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行；**
    - 8）可并行的近似直方图算法，树结点在进行分裂时，需要计算每个节点的增益，若数据量较大，对所有节点的特征进行排序，遍历的得到最优分割点，这种贪心法异常耗时，这时引进近似直方图算法，用于生成高效的分割点，即用分裂后的某种值减去分裂前的某种值，获得增益，为了限制树的增长，引入阈值，当增益大于阈值时，进行分裂；
    - 9) XGBoost的原生语言为C/C++，这是也是它训练速度快的一个原因。
    
    
3. 为啥xgboost应用二阶泰勒展开？
    - 比一阶泰勒展开，即梯度下降法，收敛速度更快
    - 二阶泰勒展开，即XGBoost支持自定义的损失函数，只要是能满足二阶连续可导的函数均可以作为损失函数
    
    
4. XGBoost如何寻找最优特征？是又放回还是无放回的呢？
    - XGBoost在训练的过程中给出各个特征的评分，即GiNI增益，从而表明每个特征对模型训练的重要性.。
    - XGBoost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴)。但XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。
<!-- #endregion -->

<!-- #region -->
### gbdt

- 基分类器是CART(classification and regression tree),使用gini系数作为指标：
- GBDT是一种基于boosting集成思想的加法模型，**训练时采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。**


1. 为什么叫Gradient Boosting？
    - 因为上面提到的这个残差，**其实就是样本对于当前已有分类器的梯度**，
    - 我们再往后的学习的y是这个残差，梯度的计算取决于损失函数，如果是经典L2损失，残差就是2*(yi'-yi),



2. GBDT为什么拟合负梯度而不是残差?
    - 函数中下降最快的方向是导数方向，同理：GBDT中，损失函数减小最快的方向也是本身的导数方向。
    - 当损失函数为均方误差时，损失函数的负梯度和残差一样，但当损失函数不为均方误差时，本质上拟合的还是负梯度。
<!-- #endregion -->

## 模型对比

<!-- #region -->
### 集成学习

| 方法类别 | 代表 | 步骤
|:- |:- |:-
| Bagging | 随机森林、dropout | s1：利用自助采样法对训练集随机采样，重复进行 T 次; <br> s2：基于每个采样集训练一个基学习器，并得到 T 个基学习器； <br> s3：预测时，集体**投票决策** 。
| Boosting | adaboost，gbdt  | s1：先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；<br> s2：根据上一个基学习器的表现，调整样本权重，使分类错误的样本得到更多的关注；<br> s3：基于调整后的样本分布，训练下一个基学习器； <br> s4：测试时，对各基学习器加权得到最终结果；
| stacking | 训练一个汇总模型来对子分类器结果进行汇总, 子分类器之间没有依赖关系。



- 为什么基分类器是决策树，因为它不稳定，所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。
    - 决策树的划分准则：信息增益（划分后熵越小，信息增益越大）、信息增益比、gini系数（基尼不纯度，表示集合的不确定性）
<!-- #endregion -->

### xgboost vs GBDT
- 1. GBDT是机器学习算法，XGBoost是该算法的一种工程实现
- 2. XGBoost在使用CART作为基学习器时，加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力
- 3. GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，可以同时使用一阶和二阶导数
- 4. XGBoost支持自定义损失函数，增强了模型的扩展性
- 5. 传统的GBDT采用CART作为基学习器（也叫基分类器），
    - XGBoost支持多种类型的基学习器，包括树模型（gbtree和dart，dart为一种引入dropout的树模型）和线性模型（gblinear），默认为gbtree
- 6. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost支持对数据进列采样，即特征采样，有利于防止过拟合，同时可以减少计算量，提高训练的效率
- 7. 传统的GBDT不能支持缺失值的处理（必须填充），**XGBoost支持缺失值的处理，能够自动学习出缺失值的分裂方向（无需填充）**
- 8. 节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的。


### XGboost vs LightGBM

- 1）寻找分割点时，差异大
    - XGBoost采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，
    - LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低，但是不能找到最精确的数据分割点；
- 2）生长策略不同
    - XGBoost采用level-wise生成决策树策略，同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合，但很多叶子节点的分裂增益较低，没必要进行更进一步的分裂，这就带来了不必要的开销；
    - LightGBM采用leaf-wise生长策略，每次从当前叶子中选择增益最大的叶子进行分裂，如此循环，但会生长出更深的决策树，产生过拟合，因此 LightGBM 在leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合）。另一个比较巧妙的优化是 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。

<!-- #region -->
### lightgbm vs xgboost
- 模型精度：XGBoost和LightGBM相当。
- 训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)
- 内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)
- 缺失值特征：**XGBoost和LightGBM都可以自动处理特征缺失值**。
- 分类特征：**XGBoost不支持类别特征**，需要OneHot编码预处理。LightGBM直接支持类别特征。


- LightGBM在XGBoost上主要有3方面的优化。
    - 1、Histogram算法:直方图算法。
    - 2、GOSS算法:基于梯度的单边采样算法。
    - 3、EFB算法:互斥特征捆绑算法。
<!-- #endregion -->

<!-- #region -->
### 逻辑回归 vs 支持向量机

1. LR和SVM的联系？
    - 1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
    - 2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。
    
    
    
2. LR和SVM的区别？
    - 1、LR是参数模型，SVM是非参数模型。
    - 2、从目标函数来看，**区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss**.
        - 这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
    - 3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。**而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。**
    - 4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
    - 5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。
<!-- #endregion -->

### 逻辑回归 vs 线性回归

1. 逻辑回归与线性回归的区别和联系？
    - 个人感觉逻辑回归和线性回归首先都是广义的线性回归，
    - 其次**经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，**
    - 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
    - 逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

```python

```
